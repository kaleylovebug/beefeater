Corpus Assignment #6

WRITE UP

Section One
 To gather data, my professor gave me access to four students' responses to Quiz One questions.
 I utilized the responses to ten questions.
 I then asked these ten questions to the AI platform ChatGPT.
 Each time I asked ChatGPT the question, I would select the "Regenerate Response" option twice, for a total of three GPT responses, which I labelled GPT1,GPT2, and GPT3.
 I initially put all of the responses into a Google Sheets document. 
 From there, I copied the information into the text editor.
 In the text editor, I created a key. 
 
 KEY: 
H1- Human 1
H2- Human 2
H3- Human 3
H4- Human 4
GPT1- AI trial 1
GPT2- AI trial 2
GPT3- AI trial 3
Q1-10- Questions 1-10

From there, I titled each response with its corresponding label and pasted the responses directly into the text editor.
For some reason, my Chromebook is not allowing me to open tsv.s, although my GitHub repository is labelled a tsv.. 
I then uploaded the text to GitHub.

Section Two
	I decided to manually tag the POS directly into the GitHub editor.
  I followed the UD tagging key that was found in a link to Assignment 7. 
  I opted to use the slash format method because it was easier to type directly onto the text.
  When I was unsure of a word's POS I used ChatGPT to try to verify, but I found that the responses it gives in its labelling are highly variable. 
  I added POS tags to the first three human responses to the ten quiz questions. One problem I ran into was making sense of the UD POS tags.
  I currently take Syntax, and we utilize more broad/just different labels for many POS. 
	
Section Three
	After adding the POS tags to the linguistic data in my corpus, there a couple of potential research question avenues I could take.
  It would be interesting to explore whether or not ADJs occur more in human responses, due to them being less necessary information, and thereby potentially less likely to occur in an AI response.
  Another research question I could consider could be related to the variability in human responses compared to AI responses. 
  One measure of variability in verbal data is TTR, the type-token ratio. 
  A higher TTR indicated higher word variability across the text. 
  So a potential research question would be: Is the TTR of human test responses higher than that of ChatGPT AI responses?
