	Q1	Q2	Q3	Q4	Q5	Q6	Q7	Q8	Q9	Q10
H1	A linguistic corpus often contains a broad set of linguistic information on a variety of topics, while datasets in other fields are specific to certain instances and can be quantitative rather than linguistic. 	I was very interested in the Brown Corpus because although it was initially a very cutting-edge corpus, I thought it was fascinating that the data it contained is not adequate for many research questions in linguistics today, showing the growth of the field. 	I think that the Brown Corpus was initially created with the lofty goal of serving as a definitive collection of texts and linguistic information that could be used for research, and although it has not maintained its position as a definitive source, it is still useful for linguistic research even in the modern era. 	Even in the same language, there can be a variety of different dialects and ways of using the language, and when creating a corpus there is a lot of discretion used when choosing sources, meaning that there are a nearly infinite number of different source combinations that corpora can utilize which leads to differences. 	It is difficult to obtain legal samples of spontaneously spoken language, and transcription and annotation of this speech can be difficult and expensive.  	The Switchboard Corpus contains samples from spontaneous conversations over the phone, which is useful because it shows how speech is used in everyday life rather than in writing or formal presentations such as newscasts. 	The sources were intended to be representative of many forms of English in order to provide coverage of as many different styles of text as possible, leading to the inclusion of a variety of different genres. 	I think that this was a good choice because to the native Engish speaker ear, there can often be a level of ambiguity about how a dropped g sounds (at least in my experiences), making it difficult to distinguish between "going" and "goin" at times, so this choice leads to less bias in the way that conversations are transcribed. 	I believe that I would want to choose a corpus that focuses on spoken data rather than Project Gutenberg because "gonna" is used nearly exclusively in speech and rarely in written texts (with the exception of some dialogue). 	There seem to be a decent number of code swtiching articles and corpora, but most seem to be limited to American bilingual speakers, and there appears to be a geographic separation of corpora and studies since bilingual speakers in the Southwestern US tend to have a different use of language than speakers in areas like Miami. 
H2		The megacorpora stood out to me because of their sheer impressiveness; of these COCA stood out to me due to its claim at being the most "representative" corpus of English.	As I implied, COCA was created to be the most "representative" corpus of English and thus purposefully includes specific proportions of different kinds of media to maximize its representativeness of all English that is produced.	Every linguist thinks that the materials that *they* compile and the ways *they* annotate it are relevant to a linguistic question they want to answer; not all corpora can answer the same question in the same ways. To be specific, different corpora will include different information about the speaker/writer, the (morpho)syntactic properties of the writing/speech, etc. and this information will be used to answer different linguistic questions.	If speakers are aware of their speech being recorded, then it becomes less spontaneous and therefore less representative of how people actually talk; getting consent to record spontaneous speech is thus ethically difficult.			I think this was a fine choice; English speakers at least somewhat conceptualize "we're" as one unit due to its orthographic representation as one word, but we do not think at all about g-drop since, compared to "we're", there is no orthographic alternation.	No it would not be; gonna is a mainly spoken phenomenon and Project Gutenberg is a repository of written materials so it would be better to consult a spoken language corpus unless you are specifically researching the transition of gonna into written English.	
H3	A linguistic corpus is compiled texts or speech that serve to answer questions about linguistic phenomena.	Switchboard Dialog Act Corpus because it seems to be entirely spoken data of telephone conversations as opposed to being mainly text. This could also allow for more natural use of the language than in text. 	Allows for more natural use of the language in a spoken context than simply having people write out their conversations or obtaining data from their written texts. 	There are many different corpora even in the same language because, over time, there has been many changes to language which are worth studying and comparing. 	Transcribing the spoken conversations for spontaneous corpora can be a hassle or too extensive of a task to do accurately.	British Academic Spoken English (BASE)	A mix of sources allows for a broader understanding of the language used, as opposed to concentrating on only one type of composition (such as Science Fiction texts) where the language used is specific to that category. 	Do not think it was a good choice because it does not accurately represent the spontaneous speech of American English used by the speakers. 	Project Gutenberg might not be the best resource because it lacks contemporary texts due to copyright reasons and 'gonna' is a term that has been in usage, for the most part, in recent times. 	
H4	A linguistic corpus is generally constituted of a collection of curated linguistic data from a variety of written and (sometimes) spoken language sources for a multitude of purposes; which is contrary to the datasets of other fields in that they are generally constituted of numerical or semantic data for a specific or limited purpose.			There are a number of idiosyncrasies that exist even between dialects of the same language which some may feel is necessary to record in isolation (the meaning of the word "biscuit", for instance, is different between American and British English).	Spoken language information is generally more difficult to curate for a corpus, as it includes linguistic information (such as clitics) that may be challenging to tokenize; in addition to this, much of the recorded spoken language available for such a task is merely recited from a written work (newscast, written speeches, etc.).			Though perhaps it was a necessary omission on the part of the developers of the corpus for one reason or another, it seems to me a limitation, as the data does not, in this case, accurately represent the particulars of spoken language.	Because most of the information featured on the Gutenberg Project is in the public domain and are largely literary works, it would probably not be a good basis for such analyses, as the language used in pre-1923 literature is unlikely to contain the word "gonna", which has a more contemporary usage.	Many of the papers on the subject are not publicly available for viewing; also there is a greater than average concentration of spoken data.
										
GPT1	A linguistic corpus is a collection of written or spoken texts used for linguistic analysis, whereas a dataset in other fields generally refers to a collection of structured data used for analysis or machine learning purposes.	As an AI language model, I don't have personal opinions, but I can tell you that the corpora in Chapter 2 of a book can be subjective depending on the context and purpose, and each one has its unique features and characteristics.	The creation of the Brown Corpus was motivated by the desire to develop and test computational linguistic models and improve natural language processing applications.	There are so many different corpora, even for the same language like BE or SAE, because each corpus is designed with a specific research question or application in mind, and different types of corpora capture different linguistic features and phenomena.	Corpora of spontaneous spoken language are relatively rare because they are more difficult and costly to collect and annotate than written language corpora, and also require special ethical considerations and consent from participants.	The Santa Barbara Corpus of Spoken American English is a good corpus of spoken English because it includes a large and diverse collection of transcribed spontaneous conversations from across the United States, with detailed demographic information about the speakers and annotations for prosody, discourse, and pragmatic features.	The mix of sources in the Brown Corpus was motivated by the desire to capture a broad range of written genres and language use in American English from the 1960s, including both formal and informal writing styles, and to make the corpus representative of the language as a whole.	It was a reasonable choice for the Switchboard Corpus not to annotate phenomena like g-drop because the focus was on transcribing speech in a way that was faithful to the speaker's intended meaning and not to impose prescriptive rules of grammar or spelling.	Project Gutenberg would not be a good basis for analyzing the usage of gonna, as it primarily contains written texts from earlier time periods and genres that may not accurately reflect contemporary spoken language use.	When searching for papers and corpora on code switching of bilingual English-Spanish speakers, it is noticeable that there is a significant amount of research and resources available, indicating a growing interest in this area of study.
GPT2	A linguistic corpus is a large and structured collection of texts used for linguistic analysis, while a dataset in other fields refers to a structured collection of data used for various purposes such as machine learning or statistical analysis.	As an AI language model, I do not have personal preferences, but I can provide an observation that the Brown Corpus is notable for its balanced representation of different genres and styles of writing, making it a useful resource for linguistic research.	The creation of the Brown Corpus was motivated by the desire to study the patterns and frequencies of language use in order to improve natural language processing and computational linguistics.	There are many different corpora for the same language because different corpora serve different purposes and are created with different sampling methods, annotation standards, and text genres, which makes them suitable for various research questions and language processing tasks.	Corpora of spontaneous spoken language are relatively rare because they are more difficult and expensive to collect, transcribe, and annotate accurately compared to written corpora or scripted spoken corpora.	The Corpus of Contemporary American English (COCA) is a decent corpus of spoken English because it includes a wide range of spoken genres, transcriptions with time alignment, and a large size that allows for statistically reliable analysis.	The mix of sources in the Brown Corpus was motivated by the desire to capture a diverse range of written texts from different genres, authors, and time periods to create a representative sample of English language usage in the mid-twentieth century.	The decision to not represent phenomena like g-drop in the Switchboard Corpus was likely a deliberate choice to focus on features that are more salient in spontaneous spoken language and relevant to specific research questions, rather than trying to capture every possible linguistic variation.	Project Gutenberg would not be a good basis for analyzing the usage of gonna, as it primarily contains written texts and gonna is a feature of spoken language.	When searching for papers on and corpora of code switching of bilingual English-Spanish speakers, it becomes apparent that there is a significant amount of research and resources available, reflecting the importance and complexity of code-switching phenomena in multilingual communities.
GPT3	While a dataset in other fields refers to a structured collection of data, a linguistic corpus is a specialized dataset that includes natural language texts and annotations used for language analysis and research.	The Brown Corpus stood out to me in particular because it was one of the first large-scale electronic corpora and played a significant role in the development of computational linguistics.	The creation of the Brown Corpus was motivated by a desire to analyze and understand the patterns and structures of the English language in a data-driven and quantitative manner.	There are so many different corpora, even for the same language like BE or SAE, because each corpus may have its own specific goals, genres, sources, and time periods, which cater to the needs and interests of different research communities and NLP applications.	Corpora of spontaneous spoken language are relatively rare because they are difficult and expensive to collect, transcribe, and annotate accurately, as well as obtaining informed consent and protecting the privacy of the speakers.	The Spoken British National Corpus is a decent corpus of spoken English because it contains a large and diverse collection of spontaneous speech samples from a wide range of speakers and regions, with detailed annotations for prosody, discourse, and metadata.	The mix of sources in the Brown Corpus was motivated by the desire to capture a broad range of written genres and language use in American English from the 1960s, including both formal and informal writing styles, and to make the corpus representative of the language as a whole.	The decision not to include the g-drop phenomenon in the Switchboard Corpus was likely a good choice, as it is a relatively minor feature of spoken language and would have added significant complexity to the transcription process, potentially compromising the quality and consistency of the annotations.	No, Project Gutenberg would not be a good basis for analyzing the usage of gonna, as it consists entirely of written texts, while gonna is primarily used in spoken language.	